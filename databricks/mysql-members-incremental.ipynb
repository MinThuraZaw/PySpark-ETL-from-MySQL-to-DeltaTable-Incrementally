{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99764021-e9bd-4090-9686-6991be2a3e7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15db6aa1-256b-467b-8571-077da219a941",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace table demo_catalog.demo_schema.members(\n",
    "    id long,\n",
    "    member_id string,\n",
    "    name string,\n",
    "    email string,\n",
    "    phone string,\n",
    "    favorite_store_id int,\n",
    "    last_purchase_date date,\n",
    "    member_type_rfm string,\n",
    "    member_category string,\n",
    "    status int,\n",
    "    verified int,\n",
    "    photo_id int,\n",
    "    created_at timestamp,\n",
    "    updated_at timestamp,\n",
    "    deleted_at timestamp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f5db5e-a65c-4440-b35b-9f89c687d4e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# function to read data from MySQL, return DataFrame\n",
    "# don't add ; at the end of query\n",
    "# set useLegacyDatetimeCode to false if you want the utc timestamp from timestamp columns\n",
    "# use databricks secrets for username and passowrd\n",
    "\n",
    "def read_from_mysql(database_name, query):\n",
    "    try:\n",
    "        df = (\n",
    "            spark.read.format(\"jdbc\")\n",
    "            .option(\"driver\", \"org.mariadb.jdbc.Driver\")\n",
    "            .option(\n",
    "                \"url\",\n",
    "                f\"host_address/{database_name}?useLegacyDatetimeCode=false\",\n",
    "            )\n",
    "            .option(\"dbtable\", f\"({query}) as result\")\n",
    "            .option(\"user\", dbutils.secrets.get(\"jdbc\", \"demo_username\"))\n",
    "            .option(\"password\", dbutils.secrets.get(\"jdbc\", \"demo_password\"))\n",
    "            .load()\n",
    "        )\n",
    "    except Exception as error:\n",
    "        return error\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd03e07e-c5c6-4583-b1cd-218c16943ea6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example 1 <br> \n",
    "When the rows in table getting update, we can add updated_at column at the source table and read the changes rows with that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4bd3406-7b56-479b-9148-ebede2a3e9a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "current_timestamp = datetime.now()\n",
    "\n",
    "#get maximum updated timestamp from the existing table\n",
    "df_current = spark.sql(\"select max(updated_at) as max_timestamp from demo_catalog.demo_schema.members\")\n",
    "\n",
    "df_current = df_current.select(\"max_timestamp\").collect()[0]\n",
    "max_timestamp = df_current[\"max_timestamp\"]\n",
    "\n",
    "print(\"current timestamp:\", current_timestamp)\n",
    "print(\"max timestamp:\", max_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f5c81e8-f71f-4210-bb25-807096716a55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#read only updated rows from posdb \n",
    "query = f\"select * from demo_database.members where updated_at <= '{current_timestamp}' and updated_at >= '{max_timestamp}'\"\n",
    "\n",
    "new_data = read_from_mysql(\"demo_database\", query)\n",
    "new_data.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03474d23-fe70-4330-bdad-f24d3d90be1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if spark.conf.get(\"spark.databricks.delta.schema.autoMerge.enabled\") == \"false\":\n",
    "    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "    print(\"set to true\")\n",
    "elif spark.conf.get(\"spark.databricks.delta.schema.autoMerge.enabled\") == \"true\":\n",
    "    print(\"true\")\n",
    "else:\n",
    "    print(\"unknown value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bc43eac-c772-40d3-93c0-12c29f04b696",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forName(spark, \"demo_catalog.demo_schema.members\")\n",
    "\n",
    "(\n",
    "    deltaTable.alias(\"current\")\n",
    "    .merge(new_data.alias(\"new\"), \"new.id = current.id\")\n",
    "    .whenMatchedUpdateAll()\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebab7b4d-4e8b-43d0-b677-7a8d6489ff79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"demo_catalog.demo_schema.members\")\n",
    "df.limit(10).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff227d32-b899-40ed-b963-ebdbac838670",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Join with sales tables and get more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9917772-bfba-4577-8a10-58d093400d3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#truncate and write data only\n",
    "\n",
    "driver = \"org.mariadb.jdbc.Driver\"\n",
    "table = \"\"\n",
    "user = dbutils.secrets.get(\"jdbc\", \"username\")\n",
    "password = dbutils.secrets.get(\"jdbc\", \"password\")\n",
    "url = \"\"\n",
    "\n",
    "df = spark.table(\"\")\n",
    "\n",
    "df.write.format(\"jdbc\")\\\n",
    "    .option(\"driver\", driver)\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"dbtable\", table)\\\n",
    "    .option(\"truncate\", \"true\")\\\n",
    "    .option(\"user\", user)\\\n",
    "    .option(\"password\", password)\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd9af61a-9885-4178-b8ce-33cfe0de1785",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcceded0-c22e-46cd-b48c-7528b60570b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example 2 <br>\n",
    "When the source table is insert only, then we can use watermark table or any way to get the current max id. That way, we can read the changes rows only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc776bc8-edf6-45bf-b8c8-b91440f91b4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace table demo_catalog.demo_schema.members_watermark(\n",
    "  table_name string,\n",
    "  max_id long\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae125308-30e8-4570-a293-d9fc3867c59c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#get maximum id from the existing table\n",
    "df_current = spark.sql(\"select max_id from demo_catalog.demo_schema.members_watermark where table_name = 'members' \")\n",
    "\n",
    "\n",
    "print(\"current id:\", current_id)\n",
    "print(\"max id:\", max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d784f8b-5165-40fb-bc6c-d22faae4315e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "mysql-members-incremental",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
